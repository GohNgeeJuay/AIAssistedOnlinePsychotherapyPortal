{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Documented FYP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GohNgeeJuay/AIAssistedOnlinePsychotherapyPortal/blob/main/Documented_FYP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9Eh89n91c1M"
      },
      "source": [
        "This notebook will explain the steps we did for the Deep Learning FER. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI-W_g-y1wXT"
      },
      "source": [
        "The dataset which we will be using is from Khan et al:\n",
        "\n",
        "1) https://link.springer.com/chapter/10.1007/978-3-319-27863-6_28\n",
        "Automatic Affect Analysis: From Children to Adults.\n",
        "Rizwan Ahmed Khan, Alexandre Meyer, Saida Bouakaz.\n",
        "International Symposium on Visual Computing: ISVC 2015.\n",
        "\n",
        "This dataset contains the children images which we are going to use for our training. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RRGGgVb3RWZ"
      },
      "source": [
        "We will also use Keras-VGGFace2-ResNet50 trained model for our fine tuning stage.\n",
        "\n",
        "2) From: https://github.com/WeidiXie/Keras-VGGFace2-ResNet50. \n",
        "Cao18,\n",
        "author       = \"Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman \",\n",
        "title        = \"VGGFace2: A dataset for recognising face across pose and age\",\n",
        "booktitle    = \"International Conference on Automatic Face and Gesture Recognition, 2018.\",\n",
        "year         = \"2018\",\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aMydX7Z3t1j"
      },
      "source": [
        "The architecture for our DL is inspired by:\n",
        "\n",
        "3) Kuo, C., Lai, S., & Sarkis,M. (2018). A Compact Deep Learning Model for Robust Facial Expression Recognition. *2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2018*, 2202-2210, doi: 10.1109/CVPRW.2018.00286."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PY9MD346iI2"
      },
      "source": [
        "First steps are to preprocess the dataset for our use case. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2q8qlwjTrV1"
      },
      "source": [
        "#1) Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z76lzN4762_O"
      },
      "source": [
        "We worked collaboratively, so we store the dataset in our shared drive. Any work that we do in this shared google colab is sync with our shared drives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_3NGGcUO1m-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1949c89b-f10a-447a-a4ce-de38d2613e9a"
      },
      "source": [
        "#Sometimes need to flush and unmount to refresh the files in google colab\n",
        "from google.colab.drive import flush_and_unmount\n",
        "flush_and_unmount()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcADtqlCsYRJ",
        "outputId": "aca19d52-c9b8-43a7-fd8b-37aafa6e21c9"
      },
      "source": [
        "#Reference https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory\n",
        "#Mount the drive to access the files\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIwR2SbX7VxV"
      },
      "source": [
        "We can also enable the google colab's gpu. Unfortunately this GPU is limited so we used GPULab for our training stage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzAkilrKVPpx",
        "outputId": "f150bc97-f997-4fea-f1aa-db746baf80c5"
      },
      "source": [
        "import tensorflow as tf #Make sure GPU is enabled\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFT6YhkbTsu9"
      },
      "source": [
        "## 1.1) Load and and investigate format of the input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ytQSqUk7-tb"
      },
      "source": [
        "Next we need to investigate the data to see what we are working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkhjR6jJ7SYy",
        "outputId": "788f916d-7c53-4eb4-b017-4d36ddc2541a"
      },
      "source": [
        "#Import libraries and check version\n",
        "import sys\n",
        "import cv2\n",
        "print(cv2.__version__)\n",
        "print(sys.version)\n",
        "import pathlib\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.1.2\n",
            "3.7.11 (default, Jul  3 2021, 18:01:19) \n",
            "[GCC 7.5.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTNHzLqh8j-J"
      },
      "source": [
        "#Load the video from the drive\n",
        "test_video = cv2.VideoCapture('/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/LIRISChildrenSpontaneousFacialExpressionVideoDatabase/LIRISChildrenSpontaneousFacialExpressionVideoDatabase/videos_208/S1_disgust_1.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzGxlYlVGd5Q",
        "outputId": "871f794a-800a-4470-917c-c4b109fb48e9"
      },
      "source": [
        "# Find the number of frames for the test video\n",
        "video_length = int(test_video.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
        "print (\"Number of frames: \", video_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of frames:  197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRo-KIBhB-GC"
      },
      "source": [
        "## 1.2) Split the video into frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AS5Ht2Yz-_S_"
      },
      "source": [
        "We will input the video as still frames (pictures) into the network. Frames are extracted every 1/6 second."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOFPRkYnDNq9"
      },
      "source": [
        "#Function to split the video into frames\n",
        "#Reference: https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames\n",
        "def extractImages(pathIn, pathOut):\n",
        "    count = 0\n",
        "    vidcap = cv2.VideoCapture(pathIn)\n",
        "    success,image = vidcap.read()    #Grabs, decodes and returns the next video frame.\n",
        "    success = True\n",
        "    while success:\n",
        "        vidcap.set(cv2.CAP_PROP_POS_MSEC,(count*167))    # added this line. set() = set a property in video capture #0.167 for 6 frames per sec\n",
        "        #CAP_PROP_POS_MSEC = Current position of the video file in milliseconds.\n",
        "        success,image = vidcap.read()\n",
        "        #print ('Read a new frame: ', success)\n",
        "        if success == True:    #If have read in a new frame\n",
        "          cv2.imwrite( pathOut + \"/frame_%d_.jpg\" % count, image)     # THE CHANGE IS HERE\n",
        "        count = count + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIIXVcsvFO5K"
      },
      "source": [
        "We will call the function above and slice our images and store it in our drive folder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL0G1BnaRL6r"
      },
      "source": [
        "#Loop through video folder and create the sliced images\n",
        "#Reference: https://newbedev.com/how-to-iterate-over-files-in-a-given-directory\n",
        "directory = \"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/LIRISChildrenSpontaneousFacialExpressionVideoDatabase/LIRISChildrenSpontaneousFacialExpressionVideoDatabase/videos_208\"\n",
        "for videoName in os.listdir(directory):\n",
        "\n",
        "  sourceVideoDirectory = os.path.join(directory, videoName) \n",
        "  targetVideoDirectory = os.path.join(\"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/SlicedImages/\", videoName)\n",
        "  \n",
        "  #Create the target video directory if does not exist. Reference: https://stackoverflow.com/questions/273192/how-can-i-safely-create-a-nested-directory-in-python\n",
        "  pathlib.Path(targetVideoDirectory).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  #Split the video into frames. \n",
        "  extractImages(sourceVideoDirectory, targetVideoDirectory)\n",
        "\n",
        "  print(\"Done processing\" + str(videoName))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CuDVtjRCPAk"
      },
      "source": [
        "We also tried to see if we can removed the neutral expression frames from the videos. Given that LIRIS-CSE contains videos that are not peak expression, it might contain frames where there are neutral expression. \n",
        "\n",
        "This is not used because the FER used here failed to detect expressions. Possible reasons could include that these FER are trained on posed and peak expressions. We will just proceed with the next preprocessing step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xeY3fSta4S7"
      },
      "source": [
        "#Testing whether can only get the faces with the desired emotion in the frames. \n",
        "#Reference: https://analyticsindiamag.com/face-emotion-recognizer-in-6-lines-of-code/\n",
        "!pip install fer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjKAz8pFbHFE"
      },
      "source": [
        "from fer import FER\n",
        "import matplotlib.pyplot as plt \n",
        "import os\n",
        "\n",
        "rootdir = \"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/Backup/S1_disgust_1.mp4\"\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "  for file in files:\n",
        "    print(file)\n",
        "    img = plt.imread(os.path.join(subdir,file))\n",
        "    detector = FER(mtcnn=True)\n",
        "    print(detector.detect_emotions(img))\n",
        "    plt.imshow(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaYgh746ho3b"
      },
      "source": [
        "rootdir = \"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/Images/S5_surprise_1.mp4\"\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "  for file in files:\n",
        "    print(file)\n",
        "    img = plt.imread(os.path.join(subdir,file))\n",
        "    detector = FER(mtcnn=True)\n",
        "    print(detector.detect_emotions(img))\n",
        "    plt.imshow(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UypkOf0oiHba"
      },
      "source": [
        "#1.2) Extracting face from image and resizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCxxeqQmCC_n"
      },
      "source": [
        "Next, we will extract the faces from the sliced images and resize them to fit the input of our network. The face extraction should help eliminate the background which can improve the accuracy. \n",
        "\n",
        "The trained network we are using takes input of (224,224,3) which means each image needs to have height of 224, width of 224 and it has 3 channels (rgb color). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZPC2InBLwU8"
      },
      "source": [
        "#Reference: https://www.geeksforgeeks.org/cropping-faces-from-images-using-opencv-python/\n",
        "#Kuo et al used alpha = 1.05 for extracting faces\n",
        "from google.colab.patches import cv2_imshow\n",
        "def extract_face_2(image_path, destination_path, min_size):\n",
        "  '''\n",
        "  image_path = Path to the image to extract the image\n",
        "  destination_path = Path to the destination where the extracted image will be stored \n",
        "  '''\n",
        "\n",
        "  img = cv2.imread(image_path)\n",
        "\n",
        "  # Convert into grayscale\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "  \n",
        "\n",
        "  faces = face_cascade.detectMultiScale(gray, scaleFactor = 1.05, minNeighbors = 5, minSize= min_size )\n",
        "  for (x, y, w, h) in faces:\n",
        "    faces = img[y:y + h, x:x + w]\n",
        "    cv2.imwrite(destination_path, faces)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaSsmaMrlLkT"
      },
      "source": [
        "#Extract the faces from all the images\n",
        "rootSourceDir = \"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/SlicedImages\"\n",
        "rootTargetDir = rootSourceDir+\"Extracted\"\n",
        "\n",
        "pathlib.Path(rootTargetDir).mkdir(parents=True, exist_ok=True) \n",
        "\n",
        "flag = False\n",
        "\n",
        "for subdir, dirs, files in os.walk(rootSourceDir):\n",
        "  if \"S12\" in subdir or \"S11\" in subdir or \"S4\" in subdir: \n",
        "    flag = True\n",
        "\n",
        "  for file in files:\n",
        "    \n",
        "    #Get the folder name only. Reference: https://www.studytonight.com/python-howtos/how-to-get-the-last-part-of-the-path-in-python\n",
        "    folderName = os.path.basename(os.path.normpath(subdir))\n",
        "\n",
        "    #Make the target folder if does not exist\n",
        "    targetFolder = os.path.join(rootTargetDir, folderName)\n",
        "    if os.path.isdir(targetFolder) == False:\n",
        "      pathlib.Path(targetFolder).mkdir(parents=True, exist_ok=True) \n",
        "\n",
        "    # Detect faces (S12, S11, S4 uses minSize = (200,200), rest use minSize = (150, 150)). Some subjects require different parameters. \n",
        "    if flag == True:\n",
        "      extract_face_2(os.path.join(subdir,file), os.path.join(targetFolder, file), (200,200))\n",
        "    else:\n",
        "      extract_face_2(os.path.join(subdir,file), os.path.join(targetFolder, file), (150,150))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QIy_BddHJQB"
      },
      "source": [
        "#Resizing all the images to 224x224 (required to input into the trained neural network)\n",
        "\n",
        "#Reference: https://pythonexamples.org/python-opencv-cv2-resize-image/\n",
        "rootSourceDir = \"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/SlicedImagesExtracted\"\n",
        "rootTargetDir = rootSourceDir+\"Resized\"\n",
        "\n",
        "pathlib.Path(rootTargetDir).mkdir(parents=True, exist_ok=True) \n",
        "for subdir, dirs, files in os.walk(rootSourceDir):\n",
        "  for file in files:\n",
        "\n",
        "    #Get the folder name only. Reference: https://www.studytonight.com/python-howtos/how-to-get-the-last-part-of-the-path-in-python\n",
        "    folderName = os.path.basename(os.path.normpath(subdir))\n",
        "\n",
        "    #Make the target folder if does not exist\n",
        "    targetFolder = os.path.join(rootTargetDir, folderName)\n",
        "    if os.path.isdir(targetFolder) == False:\n",
        "      pathlib.Path(targetFolder).mkdir(parents=True, exist_ok=True) \n",
        "\n",
        "\n",
        "    image = cv2.imread(os.path.join(subdir,file))\n",
        "    image = cv2.resize(image, (224,224),interpolation = cv2.INTER_AREA)\n",
        "    \n",
        "    cv2.imwrite(os.path.join(targetFolder,file), image) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie1O5bLlaW3I"
      },
      "source": [
        "# 1.3) Data Duplication & Augmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYaiV56dD8LP"
      },
      "source": [
        "Now we need to perform data duplication. One way to deal with unbalanced datasets(where the class labels are not evenly disbributed) is to use class weights during the fit function in training process. \n",
        "\n",
        "Another way is to duplicate the number of images so that the data per class are even. We will be using this because we want to ensure our total number of images per class are multiple of 6 (to input into the network which is inspired by Kuo et al. ) This will be explain later. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ToofsacaWZc"
      },
      "source": [
        "# Run this\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import keras\n",
        "from glob import glob, iglob\n",
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import pickle\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import operator\n",
        "from decimal import Decimal\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shutil"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNfvpQ-8Ky-h",
        "outputId": "00b34205-07fd-4b85-a44c-71435611f89a"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn0_WfSKGVnf"
      },
      "source": [
        "This is for data duplication. We will copy the last image until it reach a multiple of 6. This could arguably affect the accuracy seeing that we have images that are the same (the last few frames) but we don't expect it to affect our accuracy that much. Each frame is 1/6 sec later from the previous frame. Worst case scenario is that we have to duplicate 5 images. It is still plausible that a person may hold an expression for one second."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT6wySz_Dmfz"
      },
      "source": [
        "rootdir = \"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/SlicedImagesExtractedResized\"\n",
        "\n",
        "for subdir, dirs, files in os.walk(rootdir):\n",
        "  n = len(files)\n",
        "  # Calculate the # of duplicate needed\n",
        "  \n",
        "  if n % 6 != 0:\n",
        "    latest_file = files[-1]\n",
        "    latest_file_list = latest_file.split(\"_\") \n",
        "    latest_file_num = int(latest_file_list[1]) + 1 \n",
        "    latest_file_path = os.path.join(subdir,latest_file) # copy path\n",
        "    duplicate_num = 6 - (n%6)\n",
        "\n",
        "    for i in range(duplicate_num):\n",
        "      file_name = \"frame_%d_.jpg\" % latest_file_num # CHANGE OF FILE NAME\n",
        "\n",
        "\n",
        "      target_path = os.path.join(subdir,file_name)\n",
        "      # copy file\n",
        "      shutil.copyfile(latest_file_path, target_path)\n",
        "\n",
        "      latest_file_num += 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvgCUz_baM91"
      },
      "source": [
        "These code blocks below is to create the initial dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "uOGBY6b5aPAy",
        "outputId": "86489690-1dd7-4584-d68b-0cb6049b1bc3"
      },
      "source": [
        "# Create a dataframe containing complete path, dataset, label and folder, filename\n",
        "df = {'complete_path' : [], 'label' : [], 'file_name' : []}\n",
        "images_path = \"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/SlicedImagesExtractedResized\"\n",
        "folder_list = os.listdir(images_path)\n",
        "\n",
        "for i in range(1, len(folder_list)):\n",
        "  image_path = os.path.join(images_path,folder_list[i])\n",
        "  # label\n",
        "  label = folder_list[i].split('_')\n",
        "  if '.' not in label[1]:\n",
        "    label = label[1].lower()\n",
        "  else:\n",
        "    label[1] = label[1].split('.')\n",
        "    label = label[1][0].lower()\n",
        "\n",
        "  if label == \"suprise\":\n",
        "    label = \"surprise\"\n",
        "  \n",
        "  for subdir, dirs, files in os.walk(image_path):\n",
        "    for file in files:\n",
        "      file_path = os.path.join(image_path,file)\n",
        "      #print(file_path)\n",
        "      df['complete_path'].append(file_path)\n",
        "      df['label'].append(label)\n",
        "      df['file_name'].append(os.path.join(folder_list[i], file))\n",
        "\n",
        "df = pd.DataFrame(data = df)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>complete_path</th>\n",
              "      <th>label</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame0.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame4.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       complete_path  ...                    file_name\n",
              "0  /content/drive/Shareddrives/Final Year Project...  ...  S10_surprise.mp4/frame0.jpg\n",
              "1  /content/drive/Shareddrives/Final Year Project...  ...  S10_surprise.mp4/frame1.jpg\n",
              "2  /content/drive/Shareddrives/Final Year Project...  ...  S10_surprise.mp4/frame2.jpg\n",
              "3  /content/drive/Shareddrives/Final Year Project...  ...  S10_surprise.mp4/frame3.jpg\n",
              "4  /content/drive/Shareddrives/Final Year Project...  ...  S10_surprise.mp4/frame4.jpg\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MPftxLuEIe-"
      },
      "source": [
        "# Create a dataframe containing complete path, dataset, label and folder, filename\n",
        "df = {'complete_path' : [], 'label' : [], 'folder_file': [], 'folder_name' : [], 'file_code':[]}\n",
        "images_path = \"/content/drive/Shareddrives/Final Year Project/CS 2/LIRIS-CSE/SlicedImagesGrcExtractedResized\"\n",
        "folder_list = os.listdir(images_path)\n",
        "\n",
        "for i in range(1, len(folder_list)):\n",
        "  image_path = os.path.join(images_path,folder_list[i])\n",
        "  # label\n",
        "  label = folder_list[i].split('_')\n",
        "  if '.' not in label[1]:\n",
        "    label = label[1].lower()\n",
        "  else:\n",
        "    label[1] = label[1].split('.')\n",
        "    label = label[1][0].lower()\n",
        "\n",
        "  if label == \"suprise\":\n",
        "    label = \"surprise\"\n",
        "  \n",
        "  for subdir, dirs, files in os.walk(image_path):\n",
        "    for file in files:\n",
        "      file_name_list = file.split(\"_\")\n",
        "      file_path = os.path.join(image_path,file)\n",
        "\n",
        "      df['complete_path'].append(file_path)\n",
        "      df['label'].append(label)\n",
        "      df['folder_name'].append(os.path.join(folder_list[i])) \n",
        "      df['folder_file'].append(os.path.join(folder_list[i], file))\n",
        "      df['file_code'].append(int(file_name_list[1]))\n",
        "      \n",
        "\n",
        "df = pd.DataFrame(data = df)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNud5KinE85V"
      },
      "source": [
        "#We will need to sort the images such that images belonging to a category will be together. This will be useful during our training \n",
        "#where we want to input batches of images that are the same category. \n",
        "\n",
        "df.sort_values(['folder_name', 'file_code'], ascending=[True, True], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEPFLh_9a1Lf"
      },
      "source": [
        "# Observe the data \n",
        "num_pics_per_class = 7\n",
        "num_of_classes = df.label.nunique()\n",
        "classes = list(df.label.unique())\n",
        "\n",
        "fig, ax = plt.subplots(nrows=num_of_classes, ncols=num_pics_per_class, figsize=(21, 3*num_of_classes))\n",
        "ax=ax.ravel()\n",
        "for row_no, label in enumerate(classes):\n",
        "    for i in range(num_pics_per_class):\n",
        "        try:\n",
        "            image = df[df['label']==label].iloc[i].complete_path\n",
        "        except:\n",
        "            print(label, i)\n",
        "        image_string = tf.io.read_file(image)\n",
        "        image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "        axis = i + row_no * num_pics_per_class\n",
        "        ax[axis].set_title(label)\n",
        "        ax[axis].imshow(image_decoded)\n",
        "        ax[axis].set_xticklabels('')\n",
        "        ax[axis].set_yticklabels('')\n",
        "plt.tight_layout\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGjFvYrUa4JX",
        "outputId": "412818fd-bc42-4e7b-8f3b-2984fe49f0bf"
      },
      "source": [
        "# Create conversion from label to integer class\n",
        "label_to_class = dict((key, value) for value, key in enumerate(sorted(df.label.unique())))\n",
        "label_to_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'anger': 0,\n",
              " 'confusing': 1,\n",
              " 'disgust': 2,\n",
              " 'fear': 3,\n",
              " 'happy': 4,\n",
              " 'sad': 5,\n",
              " 'surprise': 6}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUxPsDEAa5_n",
        "outputId": "52f53760-38ff-4ff9-c600-57ccf658c7bd"
      },
      "source": [
        "class_to_label = dict((key, value) for key, value in enumerate(sorted(df.label.unique())))\n",
        "class_to_label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'anger',\n",
              " 1: 'confusing',\n",
              " 2: 'disgust',\n",
              " 3: 'fear',\n",
              " 4: 'happy',\n",
              " 5: 'sad',\n",
              " 6: 'surprise'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dItZvktXbLOy"
      },
      "source": [
        "num_of_classes = len(class_to_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "hXo9EvRbbcIC",
        "outputId": "cff15e53-4d07-455c-85fd-2ceb27814d7a"
      },
      "source": [
        "# Convert the label to integer class and imputed it into the dataframe\n",
        "def converter(string, label_to_class):\n",
        "    switcher = label_to_class\n",
        "    return switcher.get(string, \"Invalid class\")\n",
        "\n",
        "df['class'] = df.apply(lambda x: converter(x['label'], label_to_class), axis=1)\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>complete_path</th>\n",
              "      <th>label</th>\n",
              "      <th>file_name</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame0.jpg</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame1.jpg</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame2.jpg</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame3.jpg</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/Shareddrives/Final Year Project...</td>\n",
              "      <td>surprise</td>\n",
              "      <td>S10_surprise.mp4/frame4.jpg</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       complete_path  ... class\n",
              "0  /content/drive/Shareddrives/Final Year Project...  ...     6\n",
              "1  /content/drive/Shareddrives/Final Year Project...  ...     6\n",
              "2  /content/drive/Shareddrives/Final Year Project...  ...     6\n",
              "3  /content/drive/Shareddrives/Final Year Project...  ...     6\n",
              "4  /content/drive/Shareddrives/Final Year Project...  ...     6\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJPLxNaTbfAd",
        "outputId": "c16c2b04-59ed-4376-8671-09c071a6dd9c"
      },
      "source": [
        "df.label.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "happy        2178\n",
              "surprise     1464\n",
              "fear         1404\n",
              "sad          1260\n",
              "disgust       246\n",
              "confusing      42\n",
              "anger          24\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECVQgIpeH7P3"
      },
      "source": [
        "As can be seen from the result of the code above, the categories for each class is not balanced and we want to data augment some of the classes to have a balanced dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvTGirx2uQiy"
      },
      "source": [
        "Data Augmentation \n",
        "\n",
        "Here are some of the augmentation strageties we can apply to our data:\n",
        "\n",
        "1. Image width shifts: [-70,70]\n",
        "2. Imange height shift: 0.3\n",
        "2. Image horizontal flip: true\n",
        "3. Image rotation: 60\n",
        "4. Image brightness: [0.4,2.0]\n",
        "5. Image zoom: [0.7,1.0]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRBYXXAWuU3C"
      },
      "source": [
        "from numpy import expand_dims\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeW7c_RBFqEQ"
      },
      "source": [
        "def aug_img(data, filter):\n",
        "  to_return = []\n",
        "  # expand dimension to one sample\n",
        "  samples = expand_dims(data, 0)\n",
        "\n",
        "  # create image data augmentation generator\n",
        "  if filter == \"flip\":\n",
        "    datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "\n",
        "  elif filter == \"width_shift\":\n",
        "    datagen = ImageDataGenerator(width_shift_range=[-70,70])\n",
        "\n",
        "  elif filter == \"height_shift\":\n",
        "    datagen = ImageDataGenerator(height_shift_range=0.3)\n",
        "\n",
        "  elif filter == \"rotation\":\n",
        "    datagen = ImageDataGenerator(rotation_range=60)\n",
        "\n",
        "  elif filter == \"brightness\":\n",
        "    datagen = ImageDataGenerator(brightness_range=[0.4,2.0])\n",
        "\n",
        "  else:\n",
        "    datagen = ImageDataGenerator(zoom_range=[0.7,1.0])\n",
        "  \n",
        "  # prepare iterator\n",
        "  it = datagen.flow(samples, batch_size=1)\n",
        "  \n",
        "  for i in range(6):\n",
        "    # generate batch of images\n",
        "    batch = it.next()\n",
        "    batch = tf.convert_to_tensor(batch, dtype = tf.uint8)\n",
        "    to_return.append(batch)\n",
        "  \n",
        "  return to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsneT_xIESQA"
      },
      "source": [
        "aug_type = ['flip', 'width_shift', 'height_shift', 'rotation', 'brightness','zoom'] #augmentation will be randomised"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O0Xj71DNqUz"
      },
      "source": [
        "num_max = max(df.label.value_counts())\n",
        "max_label = df.groupby('label')['label'].min()[df.groupby('label')['folder_name'].count() == num_max]\n",
        "labels_to_max = [x for x in label_to_class.keys() if x not in max_label]\n",
        "num_to_max = df.groupby('label')['label'].count()[df.groupby('label')['folder_name'].count() != num_max].to_dict()\n",
        "num_to_max = list(num_to_max.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dvXoOX7WCII",
        "outputId": "04107fdf-f498-4b77-e09e-597e4f45549d"
      },
      "source": [
        "to_return = []\n",
        "\n",
        "for i in range(len(labels_to_max)):\n",
        "  label = labels_to_max[i]\n",
        "  filtered_df = df[df['label'] == label]\n",
        "\n",
        "  filtered_df_idx = list(filtered_df.index.values.tolist())\n",
        "\n",
        "  current_num = num_to_max[i]\n",
        "\n",
        "  while current_num < num_max:\n",
        "    random_idx = random.randint(0,len(filtered_df_idx)-1)\n",
        "    row_idx = filtered_df_idx[random_idx] \n",
        "\n",
        "    img_path = df.loc[row_idx,'complete_path']\n",
        "    image_string = tf.io.read_file(img_path)\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "\n",
        "    # augment image\n",
        "    aug_idx = random.randint(0,len(aug_type)-1)\n",
        "    arr = aug_img(image_decoded, aug_type[aug_idx])\n",
        "\n",
        "    #num_to_max[i] = num_to_max[i] + to_add\n",
        "    current_num += 6\n",
        "\n",
        "    label_append = tf.one_hot(label_to_class[label], num_of_classes)\n",
        "    for i in range(len(arr)):\n",
        "      to_return.append((arr[i],label_append))\n",
        "      \n",
        "  print(label,\":\",current_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anger : 2178\n",
            "confusing : 2178\n",
            "disgust : 2178\n",
            "fear : 2178\n",
            "sad : 2178\n",
            "surprise : 2178\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTIbIeLYIWKI"
      },
      "source": [
        "As can be seen above, our dataset is finally balanced. Now we can prepare it to feed into the network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuswkjVddkQ"
      },
      "source": [
        "Convert Original Data into np.array then combine with augmented data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9tQSmdAdRrb"
      },
      "source": [
        "filenames_list = df['complete_path'].tolist()\n",
        "labels_list = df['class'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7z33J-0eIw7"
      },
      "source": [
        "def _parse_function(filename, label):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    \n",
        "    label = tf.one_hot(label, num_of_classes)\n",
        "\n",
        "    return image_decoded, label\n",
        "\n",
        "# Create function to generate the dataset\n",
        "def dataset_generator(filenames, labels):\n",
        "    return tf.data.Dataset.from_tensor_slices((filenames, labels)).map(_parse_function)\n",
        "\n",
        "\n",
        "## Construct the dataset\n",
        "ori_df = dataset_generator(filenames_list, labels_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoWQxGPJLjfE"
      },
      "source": [
        "ori_data_list = list(ori_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da2qxpRaQR1C"
      },
      "source": [
        "# Combine aug data list with ori data list \n",
        "comb_df_list = ori_data_list + to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjDLhhy1QPsu"
      },
      "source": [
        "comb_df = pd.DataFrame(comb_df_list, columns=['image_arr', 'label_arr'])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ag3JqATjJ9p"
      },
      "source": [
        "catlist = [[] for i in range(6)] #cat short for category     \n",
        "\n",
        "category = [tf.constant([1, 0, 0, 0, 0, 0],dtype= \"float32\" ),tf.constant([0, 1, 0, 0, 0, 0],dtype= \"float32\" ),tf.constant([0, 0, 1, 0, 0, 0],dtype= \"float32\" )\n",
        "            ,tf.constant([0, 0, 0, 1, 0, 0],dtype= \"float32\" ),tf.constant([0, 0, 0, 0, 1, 0],dtype= \"float32\" ),tf.constant([0, 0, 0, 0, 0, 1],dtype= \"float32\" )]\n",
        "\n",
        "\n",
        "for j in range(len(category)):\n",
        "    for i in range(len(comb_df['label_arr'])):\n",
        "        if tf.math.equal(comb_df['label_arr'][i], category[j]).numpy().all(): #Checking which category this image belongs to\n",
        "            catlist[j].append([comb_df['image_arr'][i], comb_df['label_arr'][i]])    #Append to the correct category list  \n",
        "            \n",
        "\n",
        "#Convert each to dataframe\n",
        "for j in range(len(catlist)):\n",
        "    #print(np.asarray(catlist[j]).shape)\n",
        "    catlist[j] = pd.DataFrame(catlist[j], columns = ['image_arr','label_arr'])\n",
        "    \n",
        "combined_df = pd.concat(catlist)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCF8k3VvLwoJ"
      },
      "source": [
        "catIndex = [0,0,0,0,0,0]   #The end index for each category    ###RUN THIS\n",
        "\n",
        "count = 0\n",
        "for i in range(len(combined_df['label_arr'])):\n",
        "    for j in range(len(category)):\n",
        "        if tf.math.equal(combined_df['label_arr'].iloc[i], category[j]).numpy().all(): #Checking which category this image belongs to\n",
        "            catIndex[j] = count\n",
        "            count += 1\n",
        "            break\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWQXuepGO635",
        "outputId": "d9909935-b95b-419e-dbc6-cb6b0f0fa5a5"
      },
      "source": [
        "print(catIndex)    #Each"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2177, 4355, 6533, 8711, 10889, 13067, 15245]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYaMvjYAqsId",
        "outputId": "5ea12965-7a1c-4604-e601-0fcad24ed3a3"
      },
      "source": [
        "#Try to put 1 batch into the model    \n",
        "#So we have confirm that the array needs to be in this format: Each image needs to be in [1,244,244,3]\n",
        "#Then one set of images is [img1,img2,...img6]\n",
        "X = []\n",
        "X.append([np.expand_dims(combined_df.iloc[0].image_arr,0),np.expand_dims(combined_df.iloc[1].image_arr,0),np.expand_dims(combined_df.iloc[2].image_arr,0),np.expand_dims(combined_df.iloc[3].image_arr,0),np.expand_dims(combined_df.iloc[4].image_arr,0),np.expand_dims(combined_df.iloc[5].image_arr,0)])\n",
        "result = model.predict(X,verbose = 1) #Use verbose to get progress bar\n",
        "\n",
        "print(result)\n",
        "print(result.shape)\n",
        "print(type(result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 5s 5s/step\n",
            "[[0.15578553 0.14900102 0.13337326 0.1339922  0.12747815 0.14803176\n",
            "  0.15233806]]\n",
            "(1, 7)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obay2gMfLaDq"
      },
      "source": [
        "To make our dataset be in the consistent format as above, we run the code block below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8PSRITrLYKY"
      },
      "source": [
        "#Change all indexes with 4 dimensions to 3 dimensions\n",
        "for i in range(len(combined_df)):\n",
        "  if len(combined_df['image_arr'].iloc[i].shape) == 4:\n",
        "    combined_df['image_arr'].iloc[i] = np.squeeze(combined_df['image_arr'].iloc[i],0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iaP3jPuLsH0"
      },
      "source": [
        "Now we need to split our dataset into training/validation and testing sets. We have to do it manually because we want to feed the images in a certain way. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Ly_9ewQ-ap",
        "outputId": "ab1c495a-8889-4a54-80de-2663c8033d70"
      },
      "source": [
        "# catindex = [1505, 3011, 4517, 6023, 7529, 9035]\n",
        "# To generate the random training/validation/testing sets\n",
        "# Train = 70%, validation = 15%, test = 15%. Ratio for data sets\n",
        "# Total number of rows = 9036\n",
        "totalRows = len(combined_df)\n",
        "# 1 set = 6 images\n",
        "# 1 batch = 6 sets\n",
        "# number of images each feed\n",
        "numImgPerSet = 6\n",
        "numSetPerBatch = 6\n",
        "numImgPerFeed = numImgPerSet * numSetPerBatch\n",
        "\n",
        "# Number of blocks = 9036/36 = 251\n",
        "numBlocks = int(totalRows/numImgPerFeed)\n",
        "\n",
        "indexes = list(range(numBlocks))    \n",
        "random.shuffle(indexes)     #shuffle to give randomize splitting\n",
        "\n",
        "splitIndexes1 = int(numBlocks * 0.7)    #Create the indexes for 70% training\n",
        "splitIndexes2 = int(numBlocks * 0.85)\n",
        "\n",
        "trainIndexes = indexes[:splitIndexes1]    #split according to the slice\n",
        "validationIndexes = indexes[splitIndexes1: splitIndexes2]\n",
        "testIndexes = indexes[splitIndexes2:]\n",
        "# print(trainIndexes)    #Indexes. Each indexes is telling which block to take\n",
        "# print(validationIndexes)\n",
        "# print(testIndexes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "53\n",
            "[60, 11, 35, 40, 75, 76, 26, 34, 20, 37, 18, 50]\n",
            "[33, 41, 39, 12, 54, 21, 22, 51, 52, 49, 25, 27]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_hXBHaPL5ew"
      },
      "source": [
        "#For whatever reasons we have to restart the network, use this instead of rerunning the previous cell. To ensure no leakage of samples.\n",
        "trainIndexes = [121, 248, 219, 79, 83, 144, 150, 77, 9, 37, 213, 209, 105, 199, 173, 64, 250, 116, 149, 34, 245, 148, 169, 177, 111, 232, 80, 93, 223, 206, 233, 140, 145, 31, 23, 243, 14, 95, 181, 175, 24, 67, 165, 216, 137, 78, 227, 133, 96, 97, 159, 139, 65, 226, 195, 112, 184, 201, 40, 134, 158, 68, 51, 230, 38, 28, 62, 176, 130, 54, 161, 129, 63, 212, 118, 191, 151, 125, 192, 189, 141, 215, 183, 167, 117, 72, 235, 180, 107, 103, 220, 124, 33, 30, 217, 57, 66, 89, 71, 203, 50, 205, 6, 35, 242, 231, 224, 127, 7, 98, 234, 246, 99, 225, 155, 164, 204, 228, 94, 21, 221, 114, 25, 82, 166, 214, 8, 154, 16, 170, 27, 109, 162, 3, 106, 19, 86, 123, 58, 48, 15, 207, 120, 237, 178, 153, 146, 126, 61, 108, 74, 100, 46, 194, 244, 87, 131, 69, 84, 55, 210, 193, 10, 168, 208, 152, 2, 197, 13, 91, 92, 136, 18, 11, 185]\n",
        "validationIndexes = [45, 76, 174, 211, 179, 41, 102, 186, 90, 119, 240, 218, 101, 249, 22, 12, 53, 142, 115, 163, 60, 17, 247, 238, 239, 135, 138, 236, 147, 229, 88, 42, 43, 56, 20, 196, 188, 132]\n",
        "testIndexes =  [171, 1, 200, 182, 39, 113, 70, 157, 222, 73, 81, 36, 187, 32, 44, 122, 52, 198, 143, 59, 172, 47, 75, 202, 128, 49, 5, 190, 0, 104, 160, 241, 26, 156, 4, 110, 29, 85]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwQUXh1ZMRzu"
      },
      "source": [
        "Below is the custom generator function which we used to create the data batches that flow into the network. Just a note that this code is not functioning as expected due to bugs. Future work can go into improving and resolving the issues of the epoch/batches not behaving as expected. Currently the generator does not terminate as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6B5ajHvzRx_"
      },
      "source": [
        "class Generator():    #For combined_df \n",
        "\n",
        "    def __init__(self, listOfIndexes, indexesType):\n",
        "        self.indexes = listOfIndexes\n",
        "        self.data_size = len(listOfIndexes)\n",
        "        self.batch_size = 1  #(6 sets of 6 images). Each batch have 36 images in total\n",
        "        self.step_ep = self.data_size//(self.batch_size) \n",
        "        self.indexesType = indexesType\n",
        "        self.numEpochs = 0\n",
        "        if self.indexesType == 'train' or self.indexesType == 'validation':\n",
        "            self.maxEpochs = 20\n",
        "        else:\n",
        "            self.maxEpochs = 1\n",
        "  \n",
        "    def generate(self):    #Will return a generator object which can give the input ([img1,img2,img3,img4,img5,img6], label)\n",
        "        random.shuffle(self.indexes)\n",
        "        currentIndexInList = 0 \n",
        "\n",
        "        while True: #self.numEpochs < self.maxEpochs:\n",
        "            for i in range(self.step_ep):\n",
        "\n",
        "\n",
        "                # Our input must be formatted: ([img1,img2,img3,img4,img5,img6], label)\n",
        "\n",
        "                # Creating a list of tensors\n",
        "                X = [] #for the images\n",
        "                Y = [] #for the labels\n",
        "\n",
        "\n",
        "                if currentIndexInList >= self.data_size:    #If already finish all of the batches, move on\n",
        "                    # This is when exhausted all the dataset for 1 epoch. Shuffle and reset current index in list\n",
        "                    random.shuffle(indexes)\n",
        "                    #print('Completed all data')\n",
        "                    currentIndexInList = 0\n",
        "                    break    #TODO: bugs here. Code below does not help to terminate at the correct number of batches\n",
        "#                     if self.numEpochs == self.maxEpochs:  //This is to stop after max epochs\n",
        "#                         print(\"Ended max epochs and finished data\")\n",
        "#                         self.numEpochs = 0\n",
        "#                         return # if this is the training batch and finished max epochs\n",
        "#                     else:\n",
        "#                         self.numEpochs+=1\n",
        "#                         self.generate()\n",
        "                else:\n",
        "                    currentIndexInDataframe = self.indexes[currentIndexInList] * numImgPerFeed  #The current index in dataframe\n",
        "\n",
        "                # Get the images\n",
        "                counterNum = 0\n",
        "                for setNum in range(6):\n",
        "                    for imgNum in range(6):\n",
        "                        if setNum == 0:\n",
        "                            if len(combined_df.iloc[currentIndexInDataframe + counterNum].image_arr.shape) == 3:       #If the image dimensions 3\n",
        "                                X.append(np.expand_dims(combined_df.iloc[currentIndexInDataframe + imgNum].image_arr,0)) \n",
        "                        else:              \n",
        "                            if len(combined_df.iloc[currentIndexInDataframe + imgNum].image_arr.shape) == 3:    #If the image dimensions 3\n",
        "                                X[imgNum] = np.append(X[imgNum], np.expand_dims(combined_df.iloc[currentIndexInDataframe + counterNum].image_arr,0),0)\n",
        "                        counterNum += 1\n",
        "\n",
        "\n",
        "                    # Get labels  \n",
        "                    if setNum == 0:\n",
        "                        labels = combined_df['label_arr'].iloc[currentIndexInDataframe]\n",
        "                        labels = expand_dims(labels,0)  \n",
        "                        Y.append(labels)   \n",
        "\n",
        "\n",
        "                # Convert each img array in X into tensor\n",
        "                for i in range(6):\n",
        "                    X[i] = tf.convert_to_tensor(X[i])\n",
        "\n",
        "                currentIndexInList += 1 #Move to next batch\n",
        "                yield X,Y[0] #index 0 just to remove outer brackets\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdiSy_hpsYsC"
      },
      "source": [
        "# Create the generator for datasets \n",
        "\n",
        "# We ignore this because the fine tuning has too little data to train properly. We will reuse the training set during fine tuning. \n",
        "# splitIndexes1 = int(len(trainIndexes) * 0.8)    #Reserve 20% of training set for fine tuning.\n",
        "# trainIndexes = trainIndexes[:splitIndexes1]\n",
        "# fineTuningIndexes = trainIndexes[splitIndexes1:]\n",
        "\n",
        "testGenerator = Generator(testIndexes, \"test\")    #The generator for the test dataset. This will provide the inputs for test\n",
        "testBatches = testGenerator.generate()    #Create the generator\n",
        "trainGenerator = Generator(trainIndexes, \"train\")    \n",
        "trainBatches = trainGenerator.generate()\n",
        "validationGenerator = Generator(validationIndexes, \"validation\")    \n",
        "validationBatches = validationGenerator.generate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdYEiNjeemur"
      },
      "source": [
        "next(trainBatches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkkYwQrFaaWH"
      },
      "source": [
        "# 2. Network Training/Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRPKDriEhaZ4"
      },
      "source": [
        "Cannot use a CNN only since majority of the frames do not contain the emotion only since non-peak expression. Must use a network which contains a temporal model like RNN. Because of this we are using Kuo at al which have 6 images as input and give a prediction of the recognized emotion during that time frame.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ2AIXleh1VO"
      },
      "source": [
        "#Load the trained model ###RUN THIS AND ALL BELOW EXCEPT FOR REFERENCES\n",
        "#From: https://github.com/WeidiXie/Keras-VGGFace2-ResNet50. Note: remember to do proper citation\n",
        "# @InProceedings{Cao18,\n",
        "#   author       = \"Q. Cao, L. Shen, W. Xie, O. M. Parkhi, A. Zisserman \",\n",
        "#   title        = \"VGGFace2: A dataset for recognising face across pose and age\",\n",
        "#   booktitle    = \"International Conference on Automatic Face and Gesture Recognition, 2018.\",\n",
        "#   year         = \"2018\",\n",
        "# }\n",
        "\n",
        "from tensorflow import keras\n",
        "trainedModel = keras.models.load_model(\"/content/drive/Shareddrives/Final Year Project/CS 2/vggface2_Keras/vggface2_Keras/model/resnet50_softmax_dim512/weights.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGIAqbGDoe93"
      },
      "source": [
        "#Get the summary of trained model\n",
        "print(trainedModel.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBrpsbwzmvzX"
      },
      "source": [
        "# Freeze all the layers of trained model\n",
        "trainedModel.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YuLihY9dYyvk"
      },
      "source": [
        "#Reference: https://stackoverflow.com/questions/48018457/removing-layers-from-a-pretrained-keras-model-gives-the-same-output-as-original\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "trainedModel = Model(trainedModel.input, trainedModel.layers[-2].output) #Remove the last layer (ignoring last layer by taking 2nd last layer output).\n",
        "#TODO: Can try removing the last 2 layers.\n",
        "print(trainedModel.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsVaAlQENpS7"
      },
      "source": [
        "Below we create a custom DL network which takes the trained model and follow Kuo et al with 6 images as input and also a GRU layer just before predictions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oz-V4m8pWg8m",
        "outputId": "44e12ce2-e9d6-42cd-fdc3-34828bb54303"
      },
      "source": [
        "# Reference: \n",
        "# https://stackoverflow.com/questions/60582442/multiple-image-input-for-keras-application\n",
        "# https://stackoverflow.com/questions/58794981/is-it-possible-to-create-multiple-instances-of-the-same-cnn-that-take-in-multipl\n",
        "\n",
        "\n",
        "# Each input for each image. 224x224 rgb image\n",
        "inp1 = Input((224,224,3))\n",
        "inp2 = Input((224,224,3))\n",
        "inp3 = Input((224,224,3))\n",
        "inp4 = Input((224,224,3))\n",
        "inp5 = Input((224,224,3))\n",
        "inp6 = Input((224,224,3))\n",
        "\n",
        "# Put each image into the trained model\n",
        "out1 = trainedModel(inp1)\n",
        "out2 = trainedModel(inp2)\n",
        "out3 = trainedModel(inp3)\n",
        "out4 = trainedModel(inp4)\n",
        "out5 = trainedModel(inp5)\n",
        "out6 = trainedModel(inp6)\n",
        "\n",
        "# Since removed the last layer of the trained CNN, attach a new dense layer to classify into the 7 emotion categories.\n",
        "dense = Dense(6, activation='softmax')\n",
        "\n",
        "dout1 = dense(out1)\n",
        "dout2 = dense(out2)\n",
        "dout3 = dense(out3)\n",
        "dout4 = dense(out4)\n",
        "dout5 = dense(out5)\n",
        "dout6 = dense(out6)\n",
        "\n",
        "# Concatenating the final output\n",
        "out = Concatenate(axis=-1)([dout1, dout2, dout3, dout4, dout5, dout6])\n",
        "# Have to reshape the 2d output into a 3d output because GRU requires 3d shape.\n",
        "out = Reshape((6, 6), input_shape=(36,))(out)\n",
        "\n",
        "\n",
        "# GRU layer\n",
        "model = GRU(128)(out)\n",
        "outputs = Dense(6,activation = \"softmax\")(model)    #Give probability for each category\n",
        "model = Model([inp1,inp2,inp3,inp4,inp5,inp6], outputs)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "model (Functional)              (None, 512)          24610240    input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "                                                                 input_3[0][0]                    \n",
            "                                                                 input_4[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 7)            3591        model[0][0]                      \n",
            "                                                                 model[1][0]                      \n",
            "                                                                 model[2][0]                      \n",
            "                                                                 model[3][0]                      \n",
            "                                                                 model[4][0]                      \n",
            "                                                                 model[5][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 42)           0           dense[0][0]                      \n",
            "                                                                 dense[1][0]                      \n",
            "                                                                 dense[2][0]                      \n",
            "                                                                 dense[3][0]                      \n",
            "                                                                 dense[4][0]                      \n",
            "                                                                 dense[5][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 6, 7)         0           concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       (None, 128)          52608       reshape[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 7)            903         gru[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 24,667,342\n",
            "Trainable params: 57,102\n",
            "Non-trainable params: 24,610,240\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aJfjKOdN16_"
      },
      "source": [
        "Our model looks something like below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "-YEqvOdF3maX",
        "outputId": "348e1f03-6102-486e-9293-b6786d4a8c93"
      },
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "plot_model(model) #, to_file='/content/drive/Shareddrives/Final Year Project/CS 2/model.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTAH0qqtqJpO"
      },
      "source": [
        "#Reference: https://stats.stackexchange.com/questions/153531/what-is-batch-size-in-neural-network\n",
        "# In the neural network terminology:\n",
        "\n",
        "# one epoch = one forward pass and one backward pass of all the training examples\n",
        "# batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n",
        "# number of iterations = number of passes, each pass using [batch size] number of examples. To be clear, one pass = one forward pass + one backward pass (we do not count the forward pass and backward pass as two different passes).\n",
        "# Example: if you have 1000 training examples, and your batch size is 500, then it will take 2 iterations to complete 1 epoch."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSb4y2DDyxEh"
      },
      "source": [
        "#Train with a portion of the training data first \n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "j18rrUv4PCnJ"
      },
      "source": [
        "#Kuo: adam optimizer, 10000 iter, 2000 epochs, best validation accuracy , fixed learning rate =  0.01\n",
        "\n",
        "#Compile\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience=3, verbose=0, mode='min')\n",
        "mcp_save = ModelCheckpoint(filepath='/content/drive/Shareddrives/Final Year Project/CS 2/Deep Learning/model.{epoch:02d}-{val_loss:.2f}.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
        "\n",
        "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#Fit# Kuo: adam optimizer, 10000 iter, 2000 epochs, best validation accuracy , fixed learning rate =  0.01\n",
        "\n",
        "# Compile\n",
        "\n",
        "earlyStopping = EarlyStopping(monitor='accuracy', patience=3, verbose=0, mode='max')\n",
        "mcp_save = ModelCheckpoint(filepath='Models/checkpoint-{epoch:02d}-{accuracy:.2f}.hdf5', save_best_only=True, monitor='accuracy', mode='max')\n",
        "\n",
        "model.compile(optimizer= 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Fit  (, steps_per_epoch=trainGenerator.step_ep) omitted this <- (Another version of progress bar: TqdmCallback(verbose=2))\n",
        "history = model.fit(x = trainBatches, validation_data = validationBatches, epochs = 10,verbose=1, callbacks=[earlyStopping, mcp_save], steps_per_epoch=trainGenerator.step_ep)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(testBatches, verbose=1)\n",
        "print(\"Test acc is {}\".format(test_acc))\n",
        "\n",
        "model.save('Models/savedManually.h5')\n",
        "history = model.fit(x = trainBatches, validation_data = validationBatches, epochs = 10,verbose=1, callbacks=[earlyStopping, mcp_save], steps_per_epoch=trainGenerator.step_ep, use_multiprocessing= True)\n",
        "\n",
        "test_loss, test_acc = model.evaluate(testBatches, verbose=1)\n",
        "print(\"Test acc is {}\".format(test_acc))\n",
        "\n",
        "model.save('/content/drive/Shareddrives/Final Year Project/CS 2/Deep Learning/SavedManually/savedModel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9BQLC3ClYww"
      },
      "source": [
        "#Plotting\n",
        "fig, ax1 = plt.subplots()\n",
        "ax2 = ax1.twinx()\n",
        "ax1.plot(history.history['accuracy'], label='train accuracy', color='green', marker=\"o\")\n",
        "ax1.plot(history.history['val_accuracy'], label='valid accuracy', color='blue', marker = \"v\")\n",
        "ax2.plot(history.history['loss'], label = 'train loss', color='orange', marker=\"o\")\n",
        "ax2.plot(history.history['val_loss'], label = 'valid loss', color='red', marker = \"v\")\n",
        "ax1.legend(loc=3)\n",
        "\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Accuracy', color='g')\n",
        "ax2.set_ylabel('Loss', color='b')\n",
        "ax2.legend(loc=4)\n",
        "plt.ylim([0.6, 2.5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaAldi2LOiT-"
      },
      "source": [
        "After the training, we can load the best saved model and perform fine tuning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajI_Kie-gmlg"
      },
      "source": [
        "Potential drawbacks:\n",
        "\n",
        "1.   Not using grayscale images\n",
        "2.   Not standardizing the images (divide by certain value)\n",
        "3.  Have not done Principle Component Analysis.\n",
        "4. Did not use Weight initializer for the layers.\n",
        "5. Did not remove non peak expressions."
      ]
    }
  ]
}